# -*- coding: utf-8 -*-
"""SubSpaceJS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13TKGc6sVjhS5Ok9MTliNmm8aGU8X4xX8
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.metrics import accuracy_score, precision_score, recall_score, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split, KFold
from tensorflow.keras import layers, losses
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Model

import random
import sys
import time
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as keras_backend
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import concatenate
from sklearn.feature_selection import mutual_info_regression
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from operator import add
from numpy import asarray
import math
import scipy
import seaborn as sns
#import networkx as nx
import numpy as np
import optuna as optuna
import json


##################### Model Creation
def create_model(
                 neurons_input = 1, num_of_layers_1=1,
                  lr=0.01, moment =0.8, actF="relu", lossF="mean_squared_error"):

  model = Sequential()
  for i in range(num_of_layers_1):
    model.add(Dense(units=neurons_input, activation=actF))
    model.add(BatchNormalization(momentum=moment))
    #model.add(Dropout(0.6))
  model.add(Dense(units=1))
  opt1 = tf.keras.optimizers.Nadam(learning_rate=lr)
  model.compile(loss=lossF, optimizer=opt1, metrics=['mse','mae','mape'])
  return model
def create_model2(
                 neurons_input = 1, num_of_layers_1=1,
                  lr=0.01, moment =0.8, actF="relu", lossF="mean_squared_error", input_dim=16):

  model = Sequential()
  model.add(tf.keras.Input(shape=(input_dim,)))
  for i in range(num_of_layers_1):
    model.add(Dense(units=neurons_input, activation=actF))
    model.add(BatchNormalization(momentum=moment))
    #model.add(Dropout(0.6))
  opt1 = tf.keras.optimizers.Nadam(learning_rate=lr)
  model.compile(loss=lossF, optimizer=opt1, metrics=['mse','mae','mape'])
  return model
def custom_loss(y_true, y_pred):
  # calculating squared difference between target and predicted values
  miu = tf.reduce_mean(y_pred, 1)  
  reshaped_mean = tf.reshape(miu, (len(miu), 1)) 
  diversity_error = tf.square(tf.subtract(y_pred , reshaped_mean))
  individual_error = tf.square(y_pred - y_true)  
  loss = individual_error - diversity_error
  #tf.print(len(loss))
  return loss
def compute_rse(y,yhat):
  #y = y.ravel()
  #yhat = yhat.ravel()
  yhat = tf.cast(yhat, dtype=tf.float64)
  mu = np.mean(y)
  return np.sqrt(np.sum((y-yhat)**2))/np.sqrt(np.sum((y-mu)**2))
class EnsembleOfRegressor():
  def __init__(self, neurons_input = 1, num_of_layers_1=1,
                  lr=0.01, moment =0.8, actF="relu", lossF="mean_squared_error", input_dim=29, num_of_regressors = 10):
    super(EnsembleOfRegressor, self).__init__()
    self.feature_extractor = create_model2(neurons_input, num_of_layers_1, lr, moment, actF, lossF, input_dim)
    self.regressors =[]
    self.num_of_regressors = num_of_regressors
    for i in range(self.num_of_regressors):
      demo = create_model(neurons_input, num_of_layers_1, lr, moment, actF, lossF)
      self.regressors.append(demo)
    #feature_extractor.trainable(True)
    intermediate = Flatten()(self.feature_extractor.layers[-1].output)
    #outputs = tf.concat([relu_out, linear_out], axis=1)
    for i in range(self.num_of_regressors):
      if i==0:
        output = self.regressors[i](intermediate)
      else:
        #output  = output + regressors[i](intermediate)
        output  = tf.concat([output, self.regressors[i](intermediate)], axis =1)
    self.model = tf.keras.Model(self.feature_extractor.inputs, output)
    self.model.compile(optimizer='nadam', loss=custom_loss, metrics="mse")
  def getModel(self):
    return self.model
  def getBestRegressor(self, Input, labels):
    lowestError = 0
    predictions = self.model.predict(Input)
    regressorWisePredictions = tf.transpose(predictions)
    for i in range(self.num_of_regressors):
      if i == 0:
        lowestError = compute_rse(labels, regressorWisePredictions[i])
        self.bestRegressor =0
      else:
        if lowestError>compute_rse(labels, regressorWisePredictions[i]):
          lowestError = compute_rse(labels, regressorWisePredictions[i])
          self.bestRegressor = i
    return self.bestRegressor
  def fitData(self, Input, Labels, Epochs, Batch_size):
    self.model.fit(Input, Labels, epochs=Epochs, batch_size=Batch_size)

  def prediction(self, Input):
    predictions = self.model.predict(Input)
    regressorWisePredictions = tf.transpose(predictions)
    return regressorWisePredictions[self.bestRegressor]                            
