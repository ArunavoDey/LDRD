# -*- coding: utf-8 -*-
"""abalationNew.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GlugHkg8pViK5sUNLGUW7p0Fjdx24mfR
"""

# Commented out IPython magic to ensure Python compatibility.
# -*- coding: utf-8 -*-
"""Final Copy of  Abalation-Final  .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cm0xHEXXt2XLyHpQYATOVF_TYSYwOXfo
"""

#from google.colab import drive
#drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#!pip install optuna
#!pip install shap
#!pip install seaborn
import optuna as optuna
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
import random
import os
import sys
import time
import csv
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split, KFold
from tensorflow.keras import layers, losses
from tensorflow.keras.models import Model, Sequential, load_model
import tensorflow.keras as keras
import tensorflow.keras.backend as keras_backend
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Activation, Dropout, Flatten, Input, Dense, concatenate 
from sklearn.feature_selection import mutual_info_regression
from numpy import asarray
#from transfertools.models import LocIT, CBIT
import math
import scipy
from numpy import arange
from numpy.random import rand
from matplotlib import pyplot
import matplotlib.pyplot as plt
from matplotlib.ticker import AutoMinorLocator
import matplotlib.pyplot as plt
from operator import add
#import optunann1
import optunannPOD
import optunannVAT
import linearNet
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import xlsxwriter
import ensemRegressor
#import optunanewtransformator_1
#taskArray = []
# Commented out IPython magic to ensure Python compatibility.
def top_x(df2,variable,top_x_labels):
  for label in top_x_labels:
    df2[variable+'_'+label] = np.where(df2[variable]==label,1,0)
  return df2

def converter(df1):
  df = df1
  k = 0
  for i in range(len(df.columns)):
    if df[df.columns[i]].dtypes == 'object':
      #print("String")
      uniqueValues0 = df[df.columns[i]].unique()
      title0 = df.columns[i]
      df = top_x(df, title0, uniqueValues0)
      #encoder = OneHotEncoder(handle_unknown='ignore')
      #encoder_df = pd.DataFrame(encoder.fit_transform(data[[data.columns[i]]]).toarray())
      #final_df = pd.concat([data,encoder_df],axis=1)
      #df.drop(title0, axis=1, inplace=True)
      #title0 = data.columns[i]
      #print(title0)
      #ar = {}
      #j=0
      #for s in uniqueValues0:
        #key, value = s, j+1
        #j = j+1
        #ar[key] = value
      #print(ar)
      #data = data.replace({title0: ar})
    else:
      k = k + 1
  return df



def deleter(df1):
  df = df1
  k = 0
  deleteList =[]
  for i in range(len(df.columns)):
    if df[df.columns[i]].dtypes == 'object':
      title0 = df.columns[i]
      deleteList.append(title0)  
  for i in range(len(deleteList)):
    df.drop(deleteList[i], axis=1, inplace=True)
  return df

def norm1(x, Y):
  return ((x - Y['min']) / ((Y['max']-Y['min'])+1e-60))
def normalization(x, x1):
  train_stats = x1.describe()
  train_stats = train_stats.transpose()
  return norm1(x, train_stats)
"""
def compute_rse(y,yhat):
  #y = y.ravel()
  #yhat = yhat.ravel()
  mu = np.mean(y)
  return np.sqrt(np.sum((y-yhat)**2))/np.sqrt(np.sum((y-mu)**2))
"""
def compute_rse(y,yhat):
  #y = y.ravel()
  #yhat = yhat.ravel()
  yhat = tf.cast(yhat, dtype=tf.float64)
  mu = np.mean(y)
  return np.sqrt(np.sum((y-yhat)**2))/np.sqrt(np.sum((y-mu)**2))
def compute_smape(y,yhat):
  #y = y.ravel()
  #yhat = yhat.ravel()
  yhat = tf.cast(yhat, dtype=tf.float64)
  n = len(y)
  nr = np.abs(y - yhat)
  dr = 0.5*(np.abs(y) + np.abs(yhat))
  return (100./n)*np.sum(nr/dr)

if __name__ == "__main__":
  # Horovod: pin GPU to be used to process local rank (one GPU per process)
  gpus = tf.config.experimental.list_physical_devices('GPU')
  for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
  if gpus:
    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')
  use_msg = "Usage: " + sys.argv[0] + " input_filename"

  if len(sys.argv) < 2:
    print(use_msg)
    exit()
  #tf.debugging.set_log_device_placement(True)
  print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
  #tf.debugging.set_log_device_placement(True)
  path_to_source = sys.argv[1] #"/content/drive//MyDrive/Pascal-BFS.csv" 
  path_to_target = sys.argv[2] #"/content/drive/MyDrive/Turing-SSSP.csv"
  path_to_target2 = sys.argv[3]
  path_to_target3 = sys.argv[4]
  ep1 = int(sys.argv[5])
  trials1 = int(sys.argv[6])
  shot = sys.argv[7]
  fold = int(sys.argv[8])
  nuron_num = int(sys.argv[9])
  layers_num = int(sys.argv[10])
  lr_num = float(sys.argv[11])
  exp = int(sys.argv[12])
  exp2 = int(sys.argv[13])
  stdy = sys.argv[14]
  storageName = sys.argv[15]
  Momentum = float(sys.argv[16])
  batch_size = int(sys.argv[17])
  nuron_num2 =int(sys.argv[18]) 
  layers_num2 = int(sys.argv[19])
  lr_num2 = float(sys.argv[20])
  Momentum2 = float(sys.argv[21])
  batch_size2 = int(sys.argv[22])
  stdy2 = sys.argv[23]
  storageName2 = sys.argv[24]
  nuron_num3 =int(sys.argv[25])
  layers_num3 = int(sys.argv[26])
  lr_num3 = float(sys.argv[27])
  Momentum3 = float(sys.argv[28])
  batch_size3 = int(sys.argv[29])
  index = int(sys.argv[30])
  size = int(sys.argv[31])
  target_domain = sys.argv[32]
  testShot = sys.argv[33]
  scaler = MinMaxScaler()
  scaler2 = MinMaxScaler()
  #workbook = xlsxwriter.Workbook(f"{shot}-results.xlsx")
  #worksheet = workbook.add_worksheet()
  f1 = open(f"N-Ensemble-{shot}-{testShot}-results-of-{size}.csv", 'w')
  writer = csv.writer(f1)
  row =0
  #stdy = sys.argv[9]
  data = pd.read_csv(path_to_source)
  dataP = pd.read_csv(path_to_target)
  dataP2 = pd.read_csv(path_to_target2)
  dataP3 = pd.read_csv(path_to_target3)
  data = data.dropna(axis=0)
  dataP = dataP.dropna(axis=0)
  dataP2 = dataP2.dropna(axis=0)
  dataP3 = dataP3.dropna(axis=0)
  data = data.drop("graph_name",axis =1)
  dataP = dataP.drop("graph_name",axis =1)
  dataP2 = dataP2.drop("graph_name",axis =1)
  dataP3 = dataP3.drop("graph_name",axis=1)
  data = data.drop("d2h_time",axis =1)
  dataP = dataP.drop("d2h_time",axis =1)
  dataP2 = dataP2.drop("d2h_time",axis =1)
  dataP3 = dataP3.drop("d2h_time", axis=1)
  if exp2 == 0:
    data = data.drop("h2d_time",axis =1)
    dataP2 = dataP2.drop("h2d_time",axis =1)
    dataP3 = dataP3.drop("h2d_time",axis =1)
  elif exp2 == 1:
    data = data.drop("h2d_time",axis =1)
    dataP = dataP.drop("h2d_time",axis =1)
    dataP3 = dataP3.drop("h2d_time",axis =1)
  else:
    dataP = dataP.drop("h2d_time",axis =1)
    dataP2 = dataP2.drop("h2d_time",axis =1)
    dataP3 = dataP3.drop("h2d_time",axis =1)
  
  data = data.loc[data['kernel_time']>0]
  dataP = dataP.loc[dataP['kernel_time']>0]
  dataP2 = dataP2.loc[dataP2['kernel_time']>0]
  dataP3 = dataP3.loc[dataP3['kernel_time']>0]
  data = data.loc[data['kernel_time']<1]
  dataP = dataP.loc[dataP['kernel_time']<1]
  dataP2 = dataP2.loc[dataP2['kernel_time']<1]
  dataP3 = dataP3.loc[dataP3['kernel_time']<1]
  
  
  tm = "kernel_time"
  """
  data[tm] = np.log(data[tm])
  dataP[tm] = np.log(dataP[tm])
  dataP2[tm] = np.log(dataP2[tm])
  dataP3[tm] = np.log(dataP3[tm])
  """
  
  data_c = converter(data)
  data2_c = converter(dataP)
  data3_c = converter(dataP2)
  data4_c = converter(dataP3)
  data_c = deleter(data_c)
  data2_c = deleter(data2_c)
  data3_c = deleter(data3_c)
  data4_c = deleter(data4_c)
  
  #normalized_data = normalization(data_c, data_c)
  #normalized_data2 = normalization(data2_c, data_c)
  #normalized_data3 = normalization(data3_c, data_c)
  #normalized_data4 = normalization(data4_c, data_c)
  targetMetric1 = "kernel_time"
  targetMetric2 = "kernel_time"
  targetMetric3 = "kernel_time"
  targetMetric4 = "kernel_time"
  data_c = data_c.reset_index(drop=True)
  data2_c = data2_c.reset_index(drop=True)
  data3_c = data3_c.reset_index(drop=True)
  data4_c = data4_c.reset_index(drop=True)
  Lb = data_c[targetMetric1]
  lb = data2_c[targetMetric2]
  mb = data3_c[targetMetric3]
  nb = data4_c[targetMetric4]
  data_c1= data_c.drop(targetMetric1,axis =1)
  data2_c1= data2_c.drop(targetMetric2,axis =1)
  data3_c1= data3_c.drop(targetMetric3,axis =1)
  data4_c1= data4_c.drop(targetMetric4,axis =1)
  normalized_data = scaler.fit_transform(data_c1)
  normalized_data2 = scaler2.fit_transform(data2_c1)
  normalized_data3 = scaler.transform(data3_c1)
  normalized_data4 = scaler.transform(data4_c1)  

  X = normalized_data
  Y = Lb
  x = normalized_data2
  y = lb
  M = normalized_data3
  m = mb
  N = normalized_data4
  n = nb
  
  #f = open(f"/home1/08389/hcs77/{shot}-{target_domain}-indices.txt", "r")
  #nums = f.readlines()
  row = 0
  col = 0

  #dataset = (tf.data.Dataset.from_tensor_slices((tf.cast(X, tf.float64), tf.cast(Y, tf.float64) )) )
  #dataset = dataset.repeat().shuffle(10000).batch(128)
  try:
    with tf.device('/gpu:0'):
      testCase = 0
      while (testCase<5):
        f = open(f"/home1/08389/hcs77/N2-{shot}-{target_domain}-indices.txt", "r")
        nums = f.readlines()
        Row =[]
        X1 = tf.convert_to_tensor(X, dtype = tf.float64)
        x1 = tf.convert_to_tensor(x, dtype = tf.float64)
        xm = tf.convert_to_tensor(M, dtype = tf.float64)
        xn = tf.convert_to_tensor(N, dtype = tf.float64)
        Lb1= tf.convert_to_tensor(Lb, dtype = tf.float64)
        lb1= tf.convert_to_tensor(lb, dtype = tf.float64)
        lbm= tf.convert_to_tensor(mb, dtype = tf.float64)
        lbn= tf.convert_to_tensor(nb, dtype = tf.float64)

        x2 =[]
        lb2 = []
        dropIndices = []
        line = nums[index]
        indices = line.split()
        for i in range(size):
          rI = int(indices[i])
          if tf.math.count_nonzero(X1[rI:rI+1])>0:
            x2.append(X1[rI:rI+1])
            lb2.append(Lb1[rI:rI+1])
            dropIndices.append(rI)
        x2 = tf.convert_to_tensor( x2, dtype=tf.float64)
        lb2 = tf.convert_to_tensor( lb2, dtype=tf.float64)
        x2 = tf.reshape(x2, (x2.shape[0], x2.shape[2]))
        lb2 = tf.reshape(lb2, lb2.shape[0])
      
        td2 = data_c1.drop(labels=dropIndices, axis=0)
        tx = scaler.transform(td2)
        tx1 = tf.convert_to_tensor(tx, dtype = tf.float64)
        tlb = Lb.drop(labels=dropIndices, axis=0)
        tlb1 = tf.convert_to_tensor(tlb, dtype = tf.float64)	
      
        """
        """
        n_o_samples = int(len(x1) - (len(x1)/10))
        Ensemble = ensemRegressor.EnsembleOfRegressor(nuron_num , layers_num, lr_num, "relu", 29)

        Ensemble.fitData(x1[:n_o_samples], lb1[:n_o_samples], ep1, batch_size)
        best = Ensemble.getBestRegressor(x1[:n_o_samples], lb1[:n_o_samples])
        predictorModel = Ensemble.getModel()
        predictions = predictorModel.predict(tx1)
        predictions = tf.transpose(predictions)
 
        predictions = predictions[best]
        t2_prime = tf.cast(predictions, dtype = tf.float64)
        t2_prime = tf.reshape(t2_prime, (t2_prime.shape[0],1))
        mse = mean_squared_error(tlb1, predictions)
        rse = compute_rse(tlb1, predictions)
        smape = compute_smape(tlb1, predictions)
        new_test_Input = tf.concat([tx1, t2_prime], 1)     

        print(f"{shot}-{testShot}-Source mse is {mse}")
        print(f"{shot}-{testShot}-Source rse is {rse}")
        print(f"{shot}-{testShot}-Source smape is {smape}")
        Row.append(mse)
        Row.append(rse)
        Row.append(smape)
        #print(X1)
        predictions = predictorModel.predict(x2)
        predictions = tf.transpose(predictions)
        predictions = predictions[best]
        t2 = tf.cast(predictions, dtype = tf.float64)
        t2 = tf.reshape(t2, (t2.shape[0],1))
        #print(t2)
        new_Input = tf.concat([x2, t2], 1)
      
        #print(new_Input)
        #LFparameters = optunannPOD.finder(new_Input, Lb1, epochs = ep1, checkpoint_path=f"/home1/08389/hcs77/newLF/", num_of_trials=trials1, fold=10, stname=stdy, storageName =storageName )  #/content/MyDrive/SimpleNN/
        #combinedModel = optunannPOD.create_model(neurons_input= int(LFparameters.params['neuron']), num_of_layers_1=int(LFparameters.params['num_layers']), lr= float(LFparameters.params['lr2']), moment = float(LFparameters.params['Momentum']), actF="relu", lossF="mean_squared_error")     
        combinedModel = optunannVAT.create_model(neurons_input= nuron_num2, num_of_layers_1= layers_num2, lr= lr_num2, moment = Momentum2, actF="relu", lossF="mean_squared_error")
        """
        print("Neurons ")
        print(LFparameters.params['neuron'])
        print("Number of layers")
        print(LFparameters.params['num_layers'])
        print("Learning rate")
        print(LFparameters.params['lr2'])
        print("Best Trial Number")
        print(LFparameters.number)
        print("batch_size")
        print(LFparameters.params['batch_size'])
        print("Momentum")
        print(LFparameters.params['Momentum'])
        """
        #combinedModel.fit(new_Input, lb2, batch_size = LFparameters.params['batch_size'], epochs=1000)
        #combinedModel.save_weights(f"/home1/08389/hcs77/LFCCGrid/{shot}-model")
        """
        ii = 1
        while (3*ii) < layers_num2:
          combinedModel.layers[3*ii].set_weights(weights_list[3*ii])
          ii += 1
        """
        combinedModel.fit(new_Input, lb2, batch_size = batch_size2, epochs=1000, verbose=0)

        """
        turn =0
        while turn<1000:
          lossAr = []
          combinedModel.fit(new_Input, lb2, batch_size = batch_size2, epochs=1, verbose=0)
          p1 = combinedModel.predict(new_Input)
          rse1 = compute_rse(lb2, p1)
          lossAr.append(rse1)
          p2 = combinedModel.predict(new_test_Input)
          rse2 = compute_rse(tlb1, p2)
          lossAr.append(rse2)
          writer.writerow(lossAr)
          turn += 1
        """
        #f1.close()
        #combinedModel.load_weights(f"/home1/08389/hcs77/LFCCGrid/{shot}-model")
        predictions2 = combinedModel.predict(new_test_Input)
        t3_prime = tf.convert_to_tensor(predictions2, dtype = tf.float64)
        final_test_Input = tf.concat([tx1, t2_prime, t3_prime], 1)
        mse = mean_squared_error(tlb1, predictions2)
        rse = compute_rse(tlb1,  predictions2)
        smape = compute_smape(tlb1, predictions2)
        print(f"{shot}-{testShot}-Model2 mse is {mse}")
        print(f"{shot}-{testShot}-Model2 rse is {rse}")
        print(f"{shot}-{testShot}-Model2 smape is {smape}")
        Row.append(mse)
        Row.append(rse)
        Row.append(smape)
        predictions2 = combinedModel.predict(new_Input)
        t3 = tf.convert_to_tensor(predictions2, dtype = tf.float64)
        #print(t2)
        final_Input = tf.concat([x2, t2, t3], 1)
        #print(final_Input)
        #FinalParameters = optunannPOD.finder(final_Input, Lb1, epochs = ep1, checkpoint_path = f"/home1/08389/hcs77/newM/", num_of_trials= trials1, fold=10, stname = stdy2, storageName = storageName2)
        #FinalModel = optunannPOD.create_model(neurons_input= int(FinalParameters.params['neuron']), num_of_layers_1=int(FinalParameters.params['num_layers']), lr= float(FinalParameters.params['lr2']), moment = float(FinalParameters.params['Momentum']), actF="relu", lossF="mean_squared_error")
        FinalModel = optunannVAT.create_model(neurons_input= nuron_num3, num_of_layers_1 = layers_num3, lr= lr_num3 , moment = Momentum3, actF="relu", lossF="mean_squared_error")
        """
        print("Neurons ")
        print(FinalParameters.params['neuron'])
        print("Number of layers")
        print(FinalParameters.params['num_layers'])
        print("Learning rate")
        print(FinalParameters.params['lr2'])
        print("Best Trial Number")
        print(FinalParameters.number)
        print("batch_size")
        print(FinalParameters.params['batch_size'])
        print("Momentum")
        print(FinalParameters.params['Momentum'])
        """
        #FinalModel.fit(final_Input, lb2, batch_size = FinalParameters.params['batch_size'], epochs=1000, verbose=0)
        FinalModel.fit(final_Input, lb2, batch_size = batch_size3, epochs=1000, verbose=0)
        #FinalModel.save_weights(f"/home1/08389/hcs77/FMCCGrid/{shot}-model")
        #FinalModel.load_weights(f"/home1/08389/hcs77/FMCCGrid/{shot}-model")
        predictions3 = FinalModel.predict(final_test_Input)
        mse = mean_squared_error(tlb1, predictions3)
        rse = compute_rse(tlb1, predictions3)
        smape = compute_smape(tlb1, predictions3)
        print(f"{shot}-{testShot}-Model3 mse is {mse}")
        print(f"{shot}-{testShot}-Model3 rse is {rse}")
        print(f"{shot}-{testShot}-Model3 smape is {smape}")
        Row.append(mse)
        Row.append(rse)
        Row.append(smape)
        writer.writerow(Row)
        plt.figure(figsize=(10,10))
        plt.scatter(tlb1, predictions3)
        plt.savefig(f"/home1/08389/hcs77/Test-{shot}-{testShot}-3.pdf")
        f.close()
        index += 1
        testCase += 1
        #workbook.close()
      #print(f"Best trial number is {subspace_representation6.number}")
      #print(f"Source mse is {np.mean(mse)}")
      #print(f"Target mse is {np.mean(testMSE)}")
      f1.close()
  except RuntimeError as e:
    print(e)
